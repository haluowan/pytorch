{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning-DQN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haluowan/pytorch/blob/master/Reinforcement_Learning_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jPV_U7x7l8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuNDBYZ08HRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper parameters\n",
        "batch_size = 32\n",
        "lr = 1e-3\n",
        "epsilon = 0.9\n",
        "gamma = 0.9\n",
        "target_replace_iter = 100\n",
        "memory_capacity = 2000\n",
        "env = gym.make('CartPole-v0')\n",
        "env = env.unwrapped\n",
        "n_actions = env.action_space.n\n",
        "n_states = env.observation_space.shape[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x1hdo6P9M9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net,self).__init__()\n",
        "        self.fc1 = nn.Linear(n_states,10)\n",
        "        self.fc1.weight.data.normal_(0,0.1) # initialization\n",
        "        self.out = nn.Linear(10,n_actions)\n",
        "        self.out.weight.data.normal_(0,0.1) # initialization\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        actions_value = self.out(x)\n",
        "        return actions_value\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpuAUdYoCblw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(object):\n",
        "    def __init__(self):\n",
        "        self.eval_net,self.target_net = Net(),Net()\n",
        "        \n",
        "        self.learn_step_counter = 0                                # for target updating\n",
        "        self.memory_counter = 0                                    # for storing memory\n",
        "        self.memory = np.zeros((memory_capacity,n_states * 2 + 2))   # initialize memory\n",
        "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(),lr=lr)\n",
        "        self.loss_fuc = nn.MSELoss()\n",
        "        \n",
        "    def choose_action(self,x):\n",
        "        x = torch.unsqueeze(torch.FloatTensor(x),0)\n",
        "        # input only one sample\n",
        "        if np.random.uniform() < epsilon: # greedy\n",
        "            actions_value = self.eval_net.forward(x)\n",
        "            action = torch.max(actions_value,1)[1].data.numpy()[0,0] # return the argmax\n",
        "        else:\n",
        "            action = np.random.randint(0,n_actions)\n",
        "        return action\n",
        "\n",
        "    def store_transition(self,s,a,r,s_):\n",
        "        transition = np.hstack((s,[a,r],s_))\n",
        "        # replace the old memory with new memory\n",
        "        self.memory[index,:] = transition\n",
        "        self.memory_counter += 1\n",
        "        \n",
        "    def learn(self):\n",
        "        # target parameters update\n",
        "        if self.learn_step_counter % target_replace_iter == 0:\n",
        "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
        "        self.learn_step_counter += 1\n",
        "    \n",
        "        # sample batch transitions\n",
        "        sample_index = np.random.choice(memory_capacity,batch_size)\n",
        "        b_memory = self.memory[sample_index,:]\n",
        "        b_s = torch.FloatTensor(b_memory[:,:n_states])\n",
        "        b_a = torch.LongTensor(b_memory[:,:n_states:n_states+1].astype(int))\n",
        "        b_r = torch.FloatTensor(b_memory[:,n_states+1:n_states+2])\n",
        "        b_s_ = torch.FloatTensor(b_memory[:,-n_states:])\n",
        "\n",
        "        # q_eval w,r,t the action in experience\n",
        "        q_eval = self.eval_net(b_s).gather(1,b_a)\n",
        "        q_next = self.target_net(b_s_).detach()\n",
        "        q_target = b_r + gamma*q_next.max(1)[0]\n",
        "\n",
        "        loss = self.loss_fuc(q_eval,q_target)\n",
        "\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEubgAy4G904",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dqn = DQN()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMOwy3atMZtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i_episode in range(400):\n",
        "    s = env.reset()\n",
        "    ep_r = 0\n",
        "    while True:\n",
        "        env.render()\n",
        "        a = dqn.choose_action(s)\n",
        "        \n",
        "        # take action\n",
        "        s_,r,done,info = env.step(a)\n",
        "        \n",
        "        # modify the reward\n",
        "        x,x_dot,theta,theta_dot = s_\n",
        "        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
        "        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
        "        r = r1+r2\n",
        "        \n",
        "        dqn.store_transition(s,a,r,s_)\n",
        "        \n",
        "        ep_r += r\n",
        "        if dqn.memory_counter > memory_capacity:\n",
        "            dqn.learn()\n",
        "            if done:\n",
        "                print('Ep:',i_episode,'|Ep_r',round(ep_r,2))\n",
        "                \n",
        "                \n",
        "        if done:\n",
        "            break\n",
        "            \n",
        "        s = s_\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}